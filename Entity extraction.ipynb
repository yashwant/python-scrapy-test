{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Using Python 3, NLTK, spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing spaCy's Entity Extraction Capactiy\n",
    "\n",
    "- You should run the entity extractor on a sample of 100 sentences and report on how accurately it has been performed. \n",
    "- You do not need to describe your assessement of each sentence individually, but you should give overall statistics and use illustative examples from your sample.\n",
    "- Your analysis should be broken down by the type of named entity. \n",
    "- When an error occurs you should describe the nature of the error. You should distinguish the following cases\n",
    " - where the wrong entity type is assigned to a span\n",
    " - where the wrong span is identified\n",
    " - where an entity is missed altogether\n",
    "- A confusion matrix should be used here to summarise what you have found.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\gamer\\Documents\\resources') #Ammend to own path files in resources.zip\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "import random\n",
    "import math\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "get_ipython().magic('matplotlib inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from nltk.corpus import gutenberg\n",
    "nlp = spacy.load('en')\n",
    "#moby = gutenberg.raw('melville-moby_dick.txt')\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "#alice = gutenberg.raw('carroll-alice.txt')\n",
    "#persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "#sense = gutenberg.raw('austen-sense.txt')\n",
    "#parsed_moby = nlp(moby)\n",
    "parsed_emma = nlp(emma)\n",
    "#parsed_alice = nlp(alice)\n",
    "#parsed_persuasion = nlp(persuasion)\n",
    "#parsed_sense = nlp(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = parsed_emma\n",
    "nounphrases = [[nounphrase.text, nounphrase.root.head.text] for nounphrase in parsed_emma.noun_chunks]\n",
    "print(\"There were {} noun phrases found.\".format(len(nounphrases)))\n",
    "display(pd.DataFrame(nounphrases))\n",
    "#To remove \\n, edit to code using re.sub(\"\\s+\",\" \",<string>), to substitute all substrings consisting of one or more whitespace characters (captured with the regular expression \"\\s+\") with a single space character, \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(164746) \n",
    "sample_size = 20\n",
    "my_sample = random.sample(list(parsed_emma.sents),sample_size) # select a random sample of sentences\n",
    "for sent in my_sample:\n",
    "    sent = re.sub(\"\\s+\",\" \",sent.text) # clean up the whitespace\n",
    "    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Classifier\n",
    "\n",
    "\n",
    "- You should present the code of your gender classifier and explain how it works.\n",
    "- You should use the names.csv data (found within resources.zip).\n",
    "- Your code should deal with cases where a character is referred to by more than just their first name (e.g. \"John Jones\").\n",
    "- Your code should deal with cases where a character is referred to using a title (e.g. Mrs Smith\").\n",
    "- By running your gender classifier on a sample of data and reviewing the results, provide an indication of how accurate your gender classifier is. What proportion of names are being correctly analysed?\n",
    "- Deal with situations where just a surname is used (e.g. \"Smith\") after the gender of that character has been revealed (e.g. \"Mrs Smith\") before.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, define a function guess_gender(name,gender_map) that returns gender_map[name] when we (think we) know the gender of name, but when name does not appear in gender_map (i.e. maps to the 'unknown') it strips off all but the first token of name and tries that instead.\n",
    "\n",
    "- Second, write a function extend_gender_map(gender_map) that returns a gender map with additional mappings added for as many male and female titles as you can think of.\n",
    "\n",
    "- Third, adapt the line in the above cell\n",
    "\n",
    "     names_with_gender = [(name,gender_map[name.lower()]) for name,count in named_entity_counts(text,entity_type).most_common(number_of_entities)]\n",
    "\n",
    "    to use your guess_gender function rather than directly applying gender_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gender_map(dict_reader):\n",
    "    names_info = defaultdict(lambda: {\"gender\":\"\", \"freq\": 0.0})\n",
    "    for row in input_file:\n",
    "        name = row[\"name\"].lower()\n",
    "        if names_info[name][\"freq\"] < float(row[\"freq\"]): # is this gender more frequent?\n",
    "            names_info[name][\"gender\"] = row[\"gender\"] \n",
    "            names_info[name][\"freq\"] = float(row[\"freq\"])\n",
    "    gender_map = defaultdict(lambda: \"unknown\")\n",
    "    for name in names_info:\n",
    "        gender_map[name] = names_info[name][\"gender\"]\n",
    "    return gender_map\n",
    "\n",
    "input_file = csv.DictReader(open(\"names.csv\"))\n",
    "gender_map = create_gender_map(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def named_entity_counts(document,named_entity_label):\n",
    "    occurrences = [ent.string.strip() for ent in document.ents\n",
    "                   if ent.label_ == named_entity_label and ent.string.strip()]\n",
    "    return Counter(occurrences)\n",
    "\n",
    "text = parsed_emma\n",
    "entity_type = 'PERSON'\n",
    "number_of_entities = 10\n",
    "names_with_gender = [(name,gender_map[name.lower()]) for name,count in named_entity_counts(text,entity_type).most_common(number_of_entities)]\n",
    "display(pd.DataFrame(names_with_gender,columns=[\"Name\",\"Gender\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building feature sets that characterise the way a character is portrayed\n",
    "\n",
    "\n",
    "- You should explore a number of alternative ways of characterising the way a person in portrayed by a novelist. This should include code that (more detail on what to do for the first 2 is below):\n",
    " - Finding the entities within a novel.\n",
    " - Finding the main characters of a novel.\n",
    " - Finding the least occuring character within a novel.\n",
    "- You should describe the code that you have written to create feature sets for characters.\n",
    "- You should describe the code that showed how you were able to extract features in situations where one of the pronouns \"he\", \"she\", \"his\" and \"her\" is used in a novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\gamer\\Documents\\resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "import random\n",
    "import math\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import spacy\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from nltk.corpus import gutenberg\n",
    "nlp = spacy.load('en')\n",
    "from GutenbergCorpus import GutenbergCorpusReader as gcr\n",
    "reader = gcr.GutenbergCorpusReader()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for author in authors:\n",
    "    print(\"{0}: {1}\".format(author,len(authors[author])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "works = reader.get_authors_works('Roosevelt, Theodore')  # replace <AUTHOR NAME> by a string that is the name of an author \n",
    "for work in works:\n",
    "    print(work[\"title\"])\n",
    "#Name should be in the form of comma and space separated string, with each part of the name capitalised(e.g. authors['Walker, Anne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In this cell run spaCy on a random novel by your chosen author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_Middlemarch = nlp(works[3][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the blank cell below, define a function `get_entities_in(parsed_novel,entity_type)` that takes two inputs:\n",
    "- `parsed_novel` is the result of running spaCy on the raw text of some novel\n",
    "- `entity_type` is one of the spaCy entity types, e.g. \"PERSON\"\n",
    "\n",
    "The output should be a list of the text for each entity appearing in `parsed_novel` that is of type `entity_type`\n",
    "\n",
    "spaCy can sometimes return entities with an empty text representation, and you don't want to include these in the output.\n",
    "\n",
    "It is helpful to normalise the text as follows:\n",
    "- convert the text for each entity to lower case using `lower()`\n",
    "- remove any surrounding white space, using `strip()`\n",
    "\n",
    "Run your function on your parsed novel and look at the first 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your next idea is to define a function `get_main_characters(parsed_novel,num_charachters)` that takes two inputs:\n",
    "- `parsed_novel` is the result of running spaCy on the raw text of some novel\n",
    "- `num_charachters` is a positive whole number, specifying how many of the main characters should be returned\n",
    "\n",
    "The output will be a list of the `num_characters` most frequently occurring `\"PERSON\"` entities in `parsed_novel`.\n",
    "\n",
    "In the blank cell below, implement `get_main_characters`.\n",
    "- This function should make use of the `get_entities` function you have just defined\n",
    "- You can use `Counter` to produce a counter from a list of elements - try `Counter([\"a\",\"b\",\"a\",\"c\",\"b\"])`\n",
    "- Once you have a `Counter` you can use `Counter`'s `most_common` method to find the most common characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the idea from get_main_characters in order to find the amount of characters that are only mentioned once in a book aka the least common characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Feature Sets for Characters:\n",
    "\n",
    "We now turn to the issue of extracting feature sets for characters or sets of characters (The base has been done, whats written below is the parts needed doing).\n",
    "\n",
    " - write your code so that it is possible to specify any set of relations of interest, e.g. both nsubj and dobj\n",
    " - Refine your solution futher by removing the most commonly occurring verbs. Adapt a copy of the code that you have created when solving the previous exercise so that contexts involving the most common verbs are not displayed.\n",
    "       \n",
    "       - Hint: use a Counter to determine the count of each verb in a set of novels, and then use most_common(n) to find the most common n verbs.\n",
    "\n",
    "\n",
    " - Refine it further. Your goal should be to indentify other aspects of the context where a character is mentioned that you think will help to provide a richer characterisation of the way that a character is being portrayed by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_interesting_contexts(novels,rels,num_characters):\n",
    "    \n",
    "    def of_interest(ent,rels,main_characters):\n",
    "        return (ent.text.strip().lower() in main_characters \n",
    "                and ent.label_ == 'PERSON' \n",
    "                and ent.root.head.pos_ == 'VERB'\n",
    "                and ent.root.dep_ in rels)  \n",
    "\n",
    "    contexts = defaultdict(Counter)    \n",
    "    for parsed_novel in novels:\n",
    "        main_characters = get_main_characters(parsed_novel,num_characters)\n",
    "        for ent in parsed_novel.ents:\n",
    "            if of_interest(ent,rels,main_characters):\n",
    "                contexts[ent.text.strip().lower()][ent.root.head.lemma_] += 1\n",
    "    return contexts\n",
    "\n",
    "novels = {parsed_novel} #use a set here to allow for the possibility of having multiple texts\n",
    "number_of_characters_per_text = 8\n",
    "target_rels = {'nsubj'} #use set to allow for the possibility of several target dependency relations\n",
    "target_contexts = get_interesting_contexts(novels,target_rels,number_of_characters_per_text)\n",
    "display(pd.DataFrame.from_dict(target_contexts).applymap(lambda x: '' if math.isnan(x) else x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should describe the code that showed how you were able to extract features in situations where one of the pronouns \"he\", \"she\", \"his\" and \"her\" is used in a novel in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating differences in the way genders are portrayed\n",
    "\n",
    "- You should make it clear how you have aggregated feature sets across the male and female characters appearing in at least two collection of novels. These collections could be novels by  different authors, different sets of authors, or  sets of novels that were written at different periods in history. \n",
    "- You should discuss the result of measuring the cosine similarity of the aggregated male an female feature sets. The reason to consider different sets of novels is to look at differences in gender-based cosine similarity in the different collections.\n",
    "- In the last section you should have considered a number of alternative ways of deriving feature sets for characters. In this section, you should present the results of using these alternative approaches.\n",
    "- You should explain what you have done to assess the cosine similarity of pairs of features sets that are aggregated over randomly selected characters (i.e. characters that aren't split up on the basis of gender). This should provide an indication as to whether the differences you find when making a gender-based comparison are meaningful.\n",
    "- You should explain how you went about assessing what the impact would be of an imbalance in the number of male and female characters. Is there an gender imbalance in your gender-based comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Help\n",
    "#### Aggregating feature sets\n",
    "\n",
    "Once you are satisifed with the feature sets that you are able to build for a character, you are ready to undertake your analysis of the way characters are being portrayed based on gender.\n",
    "\n",
    "- Select a set of novels\n",
    "- Parse each of the novels with spaCy (this might take a while)\n",
    "- Determine the settings of any parameters that are needed by the code you have written to produce the character feature sets, e.g.\n",
    " - the number characters to consider in each novel\n",
    " - the number of most common verbs to disregard\n",
    "- Run your code that builds feature sets for characters over all of the novels under consideration\n",
    "- Build two aggregated feature sets, one for all female characters and one for all male characters\n",
    "\n",
    "In the next cell, we look at how to measure the difference between these two aggregated feature sets and how to assess whether the different you find is significant.\n",
    "\n",
    "#### Measuring the similarity of two feature sets\n",
    "\n",
    "The code cell below shows how to compare the similarity of two feature sets. This is now explained.\n",
    "\n",
    "- We are given two feature sets: `A` and `B`.\n",
    "- Initially, each feature set is represented as a `Counter` which is a dictionary where the keys are the features and each feature (key) is mapped to a positive number which corresponds to the strength (weight) of that feature. \n",
    " - feature set `A` has features `'a', 'b' and 'c'` with weights `1, 2 and 3`, respectively.\n",
    " - feature set `B` has features `'b', 'c', 'd' and 'e'` with weights `3, 4, 5 and 6`, respectively.\n",
    "- Note that they share some, but not all of their features.\n",
    "- Our goal is to represent both feature sets as lists in such a way that each position in a lists is consistently used for a particular feature\n",
    "- For example, we could use a list with 5 positions, where the weight of feature `'a'` is held in the first position, the weight of feature `'b'` is held in the second position, and so on. \n",
    " - with this scheme the feature list for `A` would be the list: `[1,2,3,0,0]`, and the feature list for `B` would be `[0,3,4,5,6]`.\n",
    "- The function `counters_to_feature_lists` takes two feature sets each of which is a `Counter` and returns two lists, one for each of the inputs, where both lists use the same feature representation.\n",
    "- In the first line of the function, the counters are added together. This is done because the keys of resulting counter (which is named `combined`) can be used to produce consistent mappings of the counters to lists - see lines 2 and 3.\n",
    "- Once consistent list representations are produced for the two feature sets, we can use the `cosine_similarity` function from `sklearn` as as a measure of how similar the lists are, and therefore, how similar the feature sets are.\n",
    "- `cosine_similarity` returns a real number between 0 and 1, with 1 indicating that the inputs are identical, and 0 indicating that the two inputs are completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = Counter({'a':1, 'b':2, 'c':3})\n",
    "B = Counter({'b':3, 'c':4, 'd':5, 'e':6})\n",
    "\n",
    "def counters_to_feature_lists(counter1,counter2):\n",
    "    combined = counter1 + counter2 \n",
    "    list1 = [counter1[key] for key in combined]\n",
    "    list2 = [counter2[key] for key in combined]\n",
    "    return list1,list2\n",
    "\n",
    "L1,L2 = counters_to_feature_lists(A,B)\n",
    "print(L1)\n",
    "print(L2)\n",
    "cosine_similarity([L1], [L2])[0,0]\n",
    "\n",
    "# Adapt for use in section 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
